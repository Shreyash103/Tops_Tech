{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Example Dataset Format:\n",
        "{\"prompt\": \"What is the capital of France?\", \"completion\": \"Paris\"}\n",
        "\n",
        "{\"prompt\": \"What is the currency of Japan?\", \"completion\": \"Yen\"}\n",
        "\n",
        "The dataset should consist of several examples (at least a few thousand) to fine-tune the model effectively."
      ],
      "metadata": {
        "id": "pmRq452kbqEX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUmW5Usabbe_",
        "outputId": "833834d5-5269-43fe-d887-9591791b3af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "# Prepare the Environment\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the key\n",
        "import openai\n",
        "\n",
        "openai.api_key = 'your API key'"
      ],
      "metadata": {
        "id": "I7cb7lmXb9HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the dataset\n",
        "response = openai.File.create(\n",
        "  file=open(\"dataset.jsonl\"),  # Path to your dataset file\n",
        "  purpose='fine-tune'\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "AM2FqCpicyq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning the model\n",
        "# Start fine-tuning\n",
        "response = openai.FineTune.create(\n",
        "  training_file=\"your_file_id\",  # Replace with the file ID returned from the file upload\n",
        "  model=\"gpt-3.5-turbo\"         # The base model you're fine-tuning\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "pgw2pLdccKiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the status of the fine-tuning job\n",
        "response = openai.FineTune.retrieve(id=\"fine-tune-job-id\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Ax-EL_dLcPbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the fine-tuned model\n",
        "response = openai.Completion.create(\n",
        "  model=\"fine-tuned-model-id\",  # Replace with your fine-tuned model ID\n",
        "  prompt=\"What is the capital of France?\",\n",
        "  max_tokens=50\n",
        ")\n",
        "print(response['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "HGcnsdOQcStR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model\n",
        "def evaluate_model(test_data, model):\n",
        "    correct = 0\n",
        "    total = len(test_data)\n",
        "\n",
        "    for test_case in test_data:\n",
        "        prompt = test_case[\"prompt\"]\n",
        "        expected_completion = test_case[\"completion\"]\n",
        "\n",
        "        # Generate model response\n",
        "        response = openai.Completion.create(\n",
        "            model=model,\n",
        "            prompt=prompt,\n",
        "            max_tokens=50\n",
        "        )\n",
        "\n",
        "        model_response = response['choices'][0]['text'].strip()\n",
        "\n",
        "        # Check if the generated response matches the expected completion\n",
        "        if model_response.lower() == expected_completion.lower():\n",
        "            correct += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct / total\n",
        "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Example usage\n",
        "test_data = [\n",
        "    {\"prompt\": \"What is the capital of France?\", \"completion\": \"Paris\"},\n",
        "    {\"prompt\": \"What is the currency of Japan?\", \"completion\": \"Yen\"},\n",
        "    # Add more test cases\n",
        "]\n",
        "\n",
        "# Use your fine-tuned model ID here\n",
        "evaluate_model(test_data, model=\"fine-tuned-model-id\")"
      ],
      "metadata": {
        "id": "5rkJ4boscTgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_performance(metrics):\n",
        "    # Assume metrics is a dictionary with keys as metric names and values as values\n",
        "    names = list(metrics.keys())\n",
        "    values = list(metrics.values())\n",
        "\n",
        "    plt.bar(names, values)\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Model Performance')\n",
        "    plt.show()\n",
        "\n",
        "# Example: plotting accuracy and F1 score\n",
        "metrics = {\"Accuracy\": 90.0, \"F1 Score\": 0.88}\n",
        "plot_performance(metrics)"
      ],
      "metadata": {
        "id": "acvQ9jacccdw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}